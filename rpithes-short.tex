%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                       rpithes-short.tex                         %
%         Template for a short thesis all in one file             %
%        (titlepage info below assumes masters degree}            %
%  Just run latex (or pdflatex) on this file to see how it looks  %
%      Be sure to run twice to get correct TOC and citations      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%
%  To produce the abstract title page followed by the abstract,
%  see the template file, "abstitle-mas.tex"
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{thesis}
\usepackage{graphicx}   % if you want to include graphics files
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{rotating}
\usepackage[square]{natbib}
\usepackage{aasmacros}
% \setcitestyle{authoryear,open={[},close={]}}
% \setcitationstyle{square}
\bibliographystyle{apj}
\setlist{nolistsep}
\graphicspath{{figures/}}

% Use the first command below if you want captions over 1 line indented.
% A side effect of this is to remove the use of bold for captions. 
% To restore bold, also include the second line below.
%\usepackage[hang]{caption}     % to indent subsequent lines of captions
%\renewcommand{\captionfont}{\bfseries} % only needed with caption package;
                                        %   otherwise bold is default)
                                        
%%%%%%%%%%%%%%%%%%%%  supply titlepage info  %%%%%%%%%%%%%%%%%%%%%
\thesistitle{\bf Performance Comparison of GPU Acceleration of\\MilkyWay@Home N-Body}
\author{Clayton Rayment}        
\degree{Master of Science}
\department{Computer Science} % provide your area of study here; e.g.,
%  "Mechanical Engineering", "Nuclear Engineering", "Physics", etc.
\signaturelines{3}
\thadviser{Carlos Varela}
\cothadviser{Heidi Newberg} %if needed
\memberone{W. Randolph Franklin} % if needed
%  For a masters project use \projadviser instead of \thadviser, 
%  and \coprojadviser and \cocoprojadviser if needed. 
\submitdate{March 2018\\(For Graduation May 2018)}        
\copyrightyear{2018}  % if date omitted, current year is used. 
%%%%%%%%%%%%%%%%%%%%%   end titlepage info  %%%%%%%%%%%%%%%%%%%%%%
      

\newcommand{\Psub}{\mathbin{\text{$\vcenter{\hbox{\textcircled{$-$}}}$}}}

\begin{document} 
\titlepage             % Print titlepage   
\copyrightpage        % optional         
\tableofcontents       % required 
\listoftables          % required if there are tables
\listoffigures         % required if there are figures

\specialhead{NOMENCLATURE}
\begin{table}[!h]
    \centering
    \caption{\label{tab:Constants} Table of constants used in algorithms}
    \begin{tabular}{|p{3cm}|p{10cm}|}
        \hline
        Constant & Description \\
        \hline\hline
        BPL & Bits Per Level: Required bits to encode one level of a tree structure. For quadtrees this value is two, and for octrees this value is three.\\
        \hline
        EFFNBODY & Total number of bodies including inserted dummy particles.\\
        \hline
        EPS2 & Softening parameter used to prevent singularities when particles are extremely close.\\
        \hline
        NBODY & Number of bodies in the simulation.\\
        \hline
        PREFIX & Partial Morton code used to identify the range of codes that a particular node covers.\\
        \hline
        RANGE & Range of particles which are contained within one node of binary tree.\\
        \hline
        SPLIT & Location of child indices in binary tree found using highest order differing bit in RANGE. Children located at SPLIT and SPLIT + 1.\\
        \hline
        TIMESTEP & Delta T for one step of the simulation.\\
        \hline
    \end{tabular}
\end{table}
\begin{table}[!h]
    \centering
    \caption{Table of functions used in algorithms}
    \label{tab:Functions}
    \begin{tabular}{|p{3cm}|p{10cm}|}
        \hline
        Function & Description \\
        \hline\hline
        CLZ(a) & Count Leading Zeroes function, counts the number of leading zeroes of a binary number starting with the MSB.\\
        \hline
        MAD(a, b, c) & Fused Multiply Add GPU function. First two arguments are multiplied, and the third argument is added after multiplication. Equivalent to $a \cdot b + c$.\\
        \hline
    \end{tabular}
\end{table}

\specialhead{ACKNOWLEDGMENT}
This work made possible with the combined efforts of the Rensselaer Polytechnic Institute Department of Physics, Applied Physics, and Astronomy, and the Department of Computer Science.
%==================================================================
\specialhead{ABSTRACT}
Presentation of an efficient GPU based N-Body octree construction algorithm for use on the MilkyWay@Home project based on parallel tree construction techniques outlined in \cite{Karras:2012}. Implementation of GPU based brute force and GPU based tree construction using space-filling curves is discussed. Performance analysis of CPU vs GPU brute force simulation, and CPU vs GPU tree construction. In-depth profiling analysis of GPU tree construction is performed, along with discussion on improvements to the algorithm to further increase performance.
%==================================================================
\chapter{INTRODUCTION}
\section{Practical Uses of N-Body Simulations}
Many practical applications of N-Body simulations exist. In this paper we focus on astronomical gravitational N-Body simulations. In particular, this will be used for dwarf galaxy tidal stream fitting in the MilkyWay@Home program. Other applications of gravitational N-Body simulations include but are not limited to the following:
\begin{enumerate}
    \item Sagittarius dwarf galaxy tidal debris stream fitting \citep{Law2005,Law2010}
    \item Milky Way dark matter profiling and mass distribution \citep{Sanderson2017}
    \item Virgo stellar stream fitting \citep{CarlinVirgo}
\end{enumerate}
\section{MilkyWay@Home}
MilkyWay@Home is a large-scale distributed parameter fitting project run on the Berkley Open Infrastructure for Network Computing (BOINC). Currently, over 20,000 users volunteer their computational resources towards the project, putting our network at just under one TFLOPS of total computing performance.  One focus of the project is computation of N-Body simulations on said volunteer computers. Because we depend on volunteer time to run computations, it is important to run as efficiently as possible on volunteered time. To this end, we currently employ Barnes-Hut and Fast Multipole Method algorithms on the CPU in order to streamline CPU efficiency. With the onset of consumer grade GPUs, however, many volunteer computers have a GPU which is currently unutilized by MilkyWay@Home N-Body. User statistics from the MilkyWay@Home database show 6800 computers from 5324 users returning successful workunits for MilkyWay@Home seperation. It is safe to assume that the vast majority of these users will run MilkyWay@Home GPU N-Body once it is released.
\section{GPU Computing}
A Graphics Processing Unit (GPU) is a major component of modern computing device. Run alongside the Central Processing Unit (CPU) the GPU is a massively parallel processor that performs many display-related computations. With the onset of modern APIs to access the GPU such as OpenCL, and the increase in floating point hardware available, it is possible to use this device to parallelize scientific computations such as the gravitational N-body problem.
\section{OpenCL}
MilkyWay@Home runs on volunteer computing power. As such, we must employ software that can be distributed to as many machines as possible. To this end, we utilize OpenCL to access graphics cards, as OpenCL is capable of running on both NVIDIA and AMD graphics cards. OpenCL provides a language similar to C called OpenCL C, and allows relatively easy access to hardware resources such as GPUs. Functions to be run on the compute device, called "kernels", are written in OpenCL C, and put in a seperate file which is then compiled at runtime for the user. OpenCL is compiled Just In Time (JIT) on the user's machine in order to fully account for any combination of hardware the user might have at runtime. This allows us to distribute one version of the code, which will be able to run across many different hardware configurations.
\section{Barnes Hut N-Body}
The Barnes-Hut N-Body algorithm \cite{BarnesHut:1986} is an $O(N\log(N))$ approximation of the brute-force N-Body simulation using a subdivided simulation space which is stored in a tree. For two dimensional simulations, this corresponds to a quadtree and for three dimensional simulations this corresponds to an octree. During the simulation, particles walk the tree, computing forces as the tree is scanned starting from the root. Hut et al. propose that if a cell, containing multiple bodies, is far enough away from the body we are currently computing forces to, then we can forgo descending the tree farther, and instead calculate the forces to that cell, using the mass enclosed, center of mass (COM). MilkyWay@Home uses a quadrupole moment correction to provide greater accuracy to the approximation.
%==================================================================
\chapter{METHODS}
\section{Brute Force CPU N-Body}
The existing brute force CPU algorithm implements an $O(N^2/P)$ algorithm where $N$ is the number of simulation bodies and $P$ is the number of available processing threads. Each particle calculates the forces enacted on it from every other particle in the simulation. This is a very laborious computation, however, and is generally disregarded in favor of the Barnes-Hut treecode N-Body algorithm.
%------------------------------------------------------------------
\section{Treecode CPU N-Body}
The current implementation of MilkyWay@Home utilizes a Barnes-Hut treecode algorithm which makes a close approximation to the brute force algorithm by ignoring groups of particles which are far away, and instead using their center of mass to perform a force calculation.

To begin, the simulation space is recursively divided into octants, creating an octree. This recursive subdivision continues untill there are either one or zero particles in each leaf of the resulting octree. Once the tree is constructed, it is threaded to vectorize force calculation using \texttt{NEXT} and \texttt{MORE} pointers.
%------------------------------------------------------------------
\section{Brute Force GPU N-Body}
Brute force GPU N-Body is very similar to the CPU algorithm described previously, with slight changes regarding GPU memory usage. To begin, a buffer containing all of the body information is created on the GPU, and data from the host machine is passed to the GPU. Once the GPU has received all of the data, simulation begins. For each timestep, the forces between all bodies are calculated and the positions and velocities are updated. OpenCL uses a queue based system for calling kernel operations on the GPU. In order to complete the required number of time steps for our simulation, we simply queue an appropriate number of kernel calls, and allow the GPU to run until there are no more kernel calls in the queue. Shown in Figure \ref{fig:GPUBruteForceAlg} is a flowchart diagramming the brute force GPU algorithm.

\begin{figure}[h]
    \caption{Brute Force algorithm flowchart showing the computational process between CPU and GPU}
    \label{fig:GPUBruteForceAlg}
    \centering
    \includegraphics[width=15cm]{bruteForceAlgorithmFlowchart}
\end{figure}

\subsection{Implementation}
Force calculation and integration for the brute force algorithm is broken down into four steps performed by three kernels:
\begin{enumerate}[noitemsep]
    \item Advance Half Velocity
    \item Advance Position
    \item Advance Half Velocity
    \item Calculate Force
\end{enumerate}
\subsubsection{Force Calculation}
Brute force GPU uses a parallel $O(N^2)$ algorithm where $N$ is the number of particles in the simulation. In parallel, each particle sums up the force enacted on it by every other particle in the simulation. Shown in Algorithm \ref{alg:ForceCalculationExact} is the algorithm used to determine the forces acting on each particle.
\begin{algorithm}
    \label{alg:ForceCalculationExact}
    \caption{Brute force force calculation algorithm: $O(N^2)$}
    \begin{algorithmic}
        \ForAll{S in Particles} in parallel
            \ForAll{P in Particles}
                \State $C1 \gets S.id < \text{NUM\_BODIES}$
                \State $C2 \gets \text{THREAD\_ID} < \text{NUM\_BODIES}$
                \State $dR2 \gets \sum_i (P.\text{pos}[i] - S.\text{pos}[i])^2 + \text{EPS2}$
                \State $dR \gets \sqrt{dR2}$
                \State $ai \gets P.mass/(dr*dr2) * c1 * c2$
                \State $S.a \gets S.a + ai$ 
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Integration}
CPU N-Body uses a half-velocity integration method. We assume constant acceleration over the timestep, which means that our average velocity will be given by:
\begin{align}
    V_{avg} &= V_i + (0.5)(\text{TIMESTEP})(a)
\end{align}
We define, then, a kernel which will advance the velocity of a particle by half of what it should be given the current acceleration. Shown below is the kernel which computes this.
Interleaved within the half velocity updates, we compute the new position of the bodies. Then, one more half velocity addition is completed to bring the particle up to its final velocity for the timestep. Finally, the new accelerations based on the new particle positions are calculated. Figure \ref{fig:GPUBruteForceAlg} shows this interleaving process in more detail.
%HALF VELOCITY:
\begin{algorithm}
    \label{alg:HalfVelocity}
    \caption{Half Velocity update algorithm: $O(N/P)$}
    \begin{algorithmic}
        \ForAll{P in Particles} in parallel
            \State $dtHalf \gets 0.5 * \text{TIMESTEP}$
            \State $P.vx \gets \text{MAD}(dtHalf, P.ax, P.vx)$
            \State $P.vy \gets \text{MAD}(dtHalf, P.ay, P.vy)$
            \State $P.vz \gets \text{MAD}(dtHalf, P.az, P.vz)$
        \EndFor
    \end{algorithmic}
\end{algorithm}
%POSITION:
\begin{algorithm}
    \label{alg:PositionUpdate}
    \caption{Position update algorithm: $O(N/P)$}
    \begin{algorithmic}
        \ForAll{P in Particles} in parallel
            \State $P.x \gets \text{MAD}(\text{TIMESTEP}, P.vx, P.x)$
            \State $P.y \gets \text{MAD}(\text{TIMESTEP}, P.vy, P.y)$
            \State $P.z \gets \text{MAD}(\text{TIMESTEP}, P.vz, P.z)$
        \EndFor
    \end{algorithmic}
\end{algorithm}
%------------------------------------------------------------------
\section{Treecode GPU N-Body}
The most difficult part of implementation of parallel treecode on the GPU is construction of the octree in parallel. Since most GPUs do not support recursive function calls, a complete paradigm shift is required. \cite{Karras:2012} propose an algorithm for parallel tree construction using Morton encoding. While \cite{Karras:2012} only discuss binary tree construction in depth, they outline the process for octree construction. The construction of the octree can be broken down into the following steps:
\begin{enumerate}[noitemsep]
    \item Compute bounding box of simulation space
    \item Encode all particle locations by a 30-bit Morton code
    \item Sort all particles based on 30-bit Morton code.
    \item Remove duplicate Morton codes using parallel compaction
    \item Construct a binary radix tree.
    \item Generate octree nodes.
    \item Link octree nodes to particles.
    \item Calculate node statistics.
    \item Thread octree nodes with next and more pointers.
\end{enumerate}
Shown in Figure \ref{fig:GPUTreecodeAlg} is a flowchart diagramming the Treecode GPU algorithm.

Once the tree has been constructed, force calculation occurs just like it would on the CPU, traversing the threaded tree starting from the root using \texttt{NEXT} and \texttt{MORE} pointers. By maximixing the amount of parallelizability, we ensure that the GPU maintains a high level of utilization and we don't waste GPU resources. The overall GPU tree construction algorithm then becomes:
\begin{enumerate}[noitemsep]
    \item Tree Construction
    \item Force Calculation
    \item Integration
\end{enumerate}
\begin{figure}[h]
    \caption{Treecode algorithm flowchart showing computational process between GPU and CPU}
    \label{fig:GPUTreecodeAlg}
    \centering
    \includegraphics[width=15cm]{treecodeAlgorithmFlowchart}
\end{figure}
\subsection{Implementation}
\subsubsection{Bounding Box}
To begin, we must first compute a bounding box of the simulation space. We use a parallel reduction opertion to find the minimum and maximum of three dimensions. To do this, all particle positions are loaded into two arrays for each dimension, one for minumum and one for maximum. Any extra array slots caused by an effective n-body count which is higher than the total n-body count is filled with \verb|DBL_MAX| for the minimum array, and \verb|-DBL_MAX| for the maximum array. A parallel comparitive reduction is then performed on each of these six arrays in order to produce a minimum and maximum bound for each dimension. Shown in Algorithm \ref{alg:BoundingBox} is the general outline of how the simulation bounds are calculated. 
\begin{algorithm}
    \label{alg:BoundingBox}
    \caption{Bounding Box algorithm}
    \begin{algorithmic}
        \For{($i < \lceil\log_2(\text{EFFNBODY})\rceil$)}
            \ForAll{$j = $} in parallel

            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}
%------------------------------------------------------------------
\subsubsection{Position Encoding}
Once the bounding values of the simulation space have been calculated, we are able to encode each particle's location within the bounding box with a 30-bit  Morton code, which tells us where along a space-filling z-order curve each particle lands. The deterministic nature of collapsing three dimensions into one dimension using this curve allows us to keep local particles near each other in the tree, while still being able to perform sorting functions. The algorithm for position encoding is as follows:
\begin{algorithm}
    \label{alg:PositionEncoding}
    \caption{Morton encoding algorithm: $O(N/P)$}
    \begin{algorithmic}
        \ForAll{P in Particles} in parallel    
            \State P.mCode $\gets$ encodeLocation(P)
        \EndFor
    \end{algorithmic}
\end{algorithm}

Since a Z-Order curve is an integer path, and we only have 30 bits of resolution, certain distributions of particles will cause a collision when two or more particles are very close together. Fortunately, the tree threading step allows us to efficiently resolve this problem, in full parallel, by taking advantage of some of the properties of both threading and distribution of sorted particles. However, before we can construct the tree, these degenerate codes must be removed from the array, which we will perform in a later step. Figure \ref{fig:MortonCurve} shows an example z-order curve at 6 bits of resolution on a quadtree, giving us 3 levels of depth. Note how if the maximum depth is reached, but two particles fall within the same cell, we will create degenerate codes.

Since encodeLocation() is an O(1) process, the entire running time of this algorithm becomes O(N/P).

% FIGURE: Z-ORDER CURVE

%------------------------------------------------------------------
\subsubsection{Particle Sort}
Now that  Morton codes have been obtained for each particle, we would like to sort the one dimensional array of encoded particles in order to effectively examine the hierarchy of codes. To do this, we implement a bitonic sorting algorithm as it is easily parallelized, and is agnostic to the underlying structure of particles it is sorting.

We sort using the 30-bit  Morton code as a key to arrange all of the other data arrays. Once we have completed bitonic sort, we are left with arrays of particles and data, which will be near their neighbors in the simulation. Degenerate  Morton codes will be located contiguously in the array, since they all share a  Morton code, which will help us later on to deal with these encoding collisions. Figure \ref{fig:BitonicSortResult} shows how data are keyed by  Morton code and sorted.
% FIGURE: Bitonic Morton Sort

% \begin{algorithm}
%     \label{alg:BitonicParticleSort}
%     \caption{Bitonic particle sorting algorithm}
%     \begin{algorithmic}
%         \For{$i < \lceil \log_2(\text{EFFNBODY})\rceil$}
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}
%------------------------------------------------------------------
\subsubsection{Body Creation}
In order to link bodies to the tree, and to preserve degenerate particle information before compaction, we fill leaf nodes with the data from the vectorized particle data. This is performed in a single kernel call and also allows us to remove any dummy bodies from the simulation before we construct the tree. A buffer is allocated at the start of the simulation with size $2 \times \text{EFFNBODY}$ which stores the particles in the first half of the array and the octree in the second half.
\begin{algorithm}
    \label{alg:ParticleCreation}
    \caption{Particle Creation: $O(N/P)$}
    \begin{algorithmic}
        \ForAll{$P in Particles$}
            \State $P.\text{data} \gets \text{data}[\text{global\_id}]$
        \EndFor
    \end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------
\subsubsection{Degenerate Code Removal}
With 30 bits of resolution in our  Morton code, this allows us to create an octree 10 levels deep. It is possible, and in fact common, for a distribution of particles to arise such that more than one particle occupies a single node in the tree. When this occurs, a Morton code collision will occur, with multiple particles being assigned the same  Morton code. While we will include these degenerate particles later on in the simulation, it is important that duplicate codes not exist during construction of the trees, as it can cause undesired structure and tree behavior. To remove dupicate Morton codes, we use a parallel compaction algorithm which is completed in three steps.

The first step is to count and mark the positions of degenerate  Morton codes. To begin, $N - 1$ threads are assigned to the array of Morton codes, skipping the $\text{0}^\text{th}$ element and beginning at the $\text{1}^\text{st}$. An offset buffer of size $N$ is cleared on the GPU to contain the array of degeneracy counts produced by the detection step. Each thread then checks whether or not its own Morton code matches that of the code one place to the left. When degenerate  Morton codes are identified, the responsible thread will place a $1$ in the array at the same index where the colliding Morton code was identified.

In the second step of parallel compaction, we perform an inclusive parallel prefix sum on the array of degeneracy counts. Once the prefix sum is complete, we now have an array of offsets which we can use to compact the original array of Morton codes.

In the final step, using the computed array of offsets, we assign $N$ threads to the original array of Morton codes. Then using the corresponding index of the offset array, we shift each value back by OFFSET places and place it into a swap buffer. Degenerate Morton codes will overwrite each other, leaving only one copy behind, and subsequent values will be shifted to the left by an appropriate amount. Once all values have been placed into the swap buffer, the original Morton code buffer is then exchanged for the new swap buffer. This ensures that no Morton codes are moved twice due to race conditions. Figure \ref{fig:CompactMortonCodes} shows this process in more detail.
%TODO: DETERMINE O()
\begin{algorithm}
    \label{alg:ParallelCompaction}
    \caption{Parallel Morton Code Compaction: $O()$}
    \begin{algorithmic}
        \ForAll{$P in Particles$}
            $P.\text{data} \gets \text{data}[\text{global\_id}]$
        \EndFor
    \end{algorithmic}
\end{algorithm}
%------------------------------------------------------------------
\subsubsection{Binary Tree Construction}
We are now ready to begin tree construction as outlined in \cite{Karras:2012}. For a binary tree, we know that we will require N-1 as many nodes as we have particles. Before simulation, we allocate a buffer of this size on the GPU.

The key to the creation of the binary tree in parallel is creating nodes directly in line with their corresponding  Morton code. The $\text{0}^\text{th}$ element will always be the root and all other nodes exist directly above the corresponding  Morton code. The two necessary values to compute for the hierarchy are the \texttt{RANGE} and the \texttt{SPLIT}. The \texttt{RANGE} is the range of particles which the node covers, while the \texttt{SPLIT} defines the location of the two children of the current node. 

The range is found by first \texttt{XOR}ing and counting leading zeros of the Morton code to the left and the Morton code to the right to find the direction. The direction will always go towards the direction with more leading zeros. The range is terminated by the next Morton code which has fewer leading zeros than the selected neighbor in the direction of the selected neighbor. 

The \texttt{SPLIT} is found by finding the highest order differing bit between the current node and itself. If the split is found to be the current node, then the node is at the bottom of the tree and must connect to a leaf. Figure \ref{fig:BinaryTreeGeneration} shows this process in more detail.

During tree construction we also calculate node deltas ($\delta$), which will be used during octree construction to determine the number of octree nodes each binary tree node contributes to the octree. The $\delta$ of a node is found by performing a \texttt{CLZ} on the \texttt{XOR} of the two  Morton codes bordering the split.
\begin{align}
    \delta &= \text{CLZ}(\text{mCode[SPLIT]} \text{ XOR } \text{mCode[SPLIT + 1]})
\end{align}
This gives us the number of matching elements starting from the most significant bit (MSB), and can be used to determine the node's depth in a quad/octree.

Finally, we also compute the center of mass (COM) of each of the binary tree nodes. This is an inefficient stop-gap computation which is used for validation of our tree structure, the effects of which are discussed in the results section. This computation will be replaced with a more efficient tree upsweep kernel which will perform the same computation in $O(N)$ time.

%TODO: write binary tree construction algo
\begin{algorithm}
    \label{alg:BinaryTreeConstruction}
    \caption{Binary tree construction algorithm: $O((N \log N) / P)$}
    \begin{algorithmic}
        \For{$i < \text{COMPACTSIZE} - 1$} in parallel
            \State n $\gets$ binaryNodes[i]
            \State n.RANGE $\gets$ findRange(MCODES, COMPACTSIZE, GLOBAL\_ID)
            \State n.SPLIT $\gets$ findSplit(MCODES, RANGE)
            \State n.DELTA $\gets$ CLZ(MCODES[SPLIT] XOR MCODES[SPLIT + 1])
            \If{SPLIT == RANGE.x}
                \State n.children[0] $\gets$ bodies[SPLIT]
                \State bodies[SPLIT] $\gets$ n
            \Else
                \State n.children[0] $\gets$ binaryNodes[SPLIT]
                \State binaryNodes[SPLIT] $\gets$ n
            \EndIf
            \If{SPLIT + 1 == RANGE.y}
                \State n.children[1] $\gets$ bodies[SPLIT + 1]
                \State bodies[SPLIT + 1] $\gets$ n
            \Else
                \State n.children[1] $\gets$ binaryNodes[SPLIT + 1]
                \State binaryNodes[SPLIT + 1] $\gets$ n
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}
%------------------------------------------------------------------
\subsubsection{Octree Node Generation}
We now have a complete hierarchy of particles in the simulation based on their 30-bit  Morton code. However, as this is a spacial 3d simulation, it would instead be beneficial to place these particles in an octree. Unlike the binary tree, which should have full occupancy, it is unknown how many octree nodes we will need. To begin, we must first examine the binary tree and determine how many octree nodes each binary tree node will contribute to the final structure.

For each binary tree node, in parallel, we must compute the number of octree nodes contributed to the octree. This is performed by calculating the level of the octree that the current binary tree node is on given its $\delta$, and subtracting from that the octree level of the parent. The formula is given by:

\begin{align}
    \label{eq:NodeContributions}
    \text{NC} &= \lfloor\frac{\delta_c}{\text{BPL}}\rfloor - \lfloor\frac{\delta_p}{\text{BPL}}\rfloor
\end{align}
where \texttt{NC} is the number of nodes contributed by the current binary tree node, and \texttt{BPL} is the number of bits per level of the corresponding tree structure. For a quadtree \texttt{BPL} equals two, as the four subdivisions can be encoded using two bits. For an octree \texttt{BPL} is three, as three bits can encode the eight subdivisions of an octree. Once we know how many octree nodes will be contributed by each binary tree node, we perform an inclusive parallel prefix sum to compute the offset array. This offset array will be used to determine the location of each octree node in the octree buffer. Figure \ref{fig:OctreeNodeContribution} shows the process of calculating the binary tree node contributions in more detail.

It is important to note that while we do calculate the number of needed octree nodes at each step, we do not perform any memory allocation. At the beginning of the simulation, before data is ever pushed into GPU buffers, we allocate a single array of size $2\times \texttt{EFFNBODY}$. The first half of this array contains all of our bodies, and the second half contains all of our octree nodes. Since we have no idea how many octree nodes will be needed at any given step of the simulation, we allocate the second half of the buffer to be of size \texttt{EFFNBODY} as the number of octree nodes will never exceed this value. This saves having to resize our memory buffer on the GPU which will slow down the simulation. At the end of each timestep, all buffers except for position and velocity are purged, which prepares them for the next step of the simulation.

Now that we know the index of each octree node, the final step is to perform an incomplete linking of the tree. That is, we will connect all octree nodes in the proper hierarchy in preperation to link to the particles.

Before we generate a hierarchy, we must first compute some basic information about each node, such as \texttt{PREFIX} and \texttt{TREELEVEL}. A node's \texttt{TREELEVEL} is the level in the tree which the node resides. For example, the root node will reside at level 0 and the maximum level of the tree is 10. The \texttt{TREELEVEL} is computed by examining the $\delta$ of the binary tree node which contributed this octree node to the structure. The level of the octree node is given by $\delta/3$. A node's \text{PREFIX} is the partial  Morton code which that node spans at its level in the tree. The \text{PREFIX} is calculated by shifting the contributing binary tree node's  Morton code to the right by \texttt{BPL} $\times$ \texttt{TREELEVEL} bits.  

Using the offset array and the basic information we have calculated, we are ready to begin generating an octree hierarcy. One special case we must consider is when a binary tree node contributes more than one octree node to the octree. In this case, we iterate over the number of nodes which will be contributed, stepping sequentially through the list of octree nodes. We start at the index given by the offset array, as these strands of octree nodes will be located contiguously in memory. The prefix of each child will be \texttt{BPL} digits longer than the prefix of its parent, and the location of the node in the parent's child array is determined by this extra chunk.

We now have both single unattached nodes and disconnected strands of nodes which need to be linked to the overall hierarchy. To determine the parent of each octree node we must inspect the binary tree. We start at the binary tree node which contributed the octree node to the octree. We then follow the parents of this node up the tree, until we reach another binary node which has contributed a node to the octree. This node is then the parent of the node we wish to link. We must take care, however, as an octree strand may be the parent of another node. In this case we must offset our found parent index by the number of nodes contributed by the found parent. Since octree strands are contiguous in memory, we can simply offset our index by the found parent's contribution to the tree. Figure \ref{fig:OctreeNodeGeneration} shows the combined process of generating and linking the octree nodes based on the binary tree hierarchy.

% Binary Tree Contributions
\begin{algorithm}
    \label{alg:NodeContributions}
    \caption{Octree node contribution algorithm: $O(N/P)$}
    \begin{algorithmic}
        \For{$i < \text{COMPACTSIZE} - 1$} in parallel
            \If{$i \neq 0$}
                \State DELTA $\gets$ binaryNodes[i].delta
                \State PARENT\_DELTA $\gets$ binaryNodes[i].parent.delta
                \State nodeCounts[i] $\gets$ DELTA/3 - PARENT\_DELTA/3
            \Else
                \State nodeCounts[i] $\gets$ 0
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}

% Now that the binary node contributions to the octree are computed, we can run the three parallel prefix sum kernels to compute the array of offsets:
% %Parallel Prefix Sumx
% First is the upsweep kernel:
% \begin{algorithm}
%     \label{alg:PrefixUpsweep}
%     \caption{Prefix sum upsweep algorithm}
%     \begin{algorithmic}
%         \For{}
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}

% Next, is the downsweep kernel:
% \begin{algorithm}
%     \label{alg:PrefixDownsweep}
%     \caption{Prefix sum downsweep algorithm}
%     \begin{algorithmic}
%         \For{}
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}

% Finally, we return the original node counts from their buffer to make the prefix sum inclusive:
% \begin{algorithm}
%     \label{alg:PrefixInclusive}
%     \caption{Prefix sum inclusive addition algorithm}
%     \begin{algorithmic}
%         \For{}
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}

%-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
%TODO: write octree generation algorithm
\begin{algorithm}
    \label{alg:GenerateOctreeNodes}
    \caption{Octree node generation algorithm: $O((N \log N)/P)$}
    \begin{algorithmic}
        \ForAll{$g < \text{COMPACTSIZE} - 1$}
            \If{$g \neq 0$}
                \State index $\gets$ nodeCounts[g - 1] + 1
                \State count $\gets$ nodeCounts[g] - nodeCounts[g-1]
                \If{count $>$ 0}
                    \For{i $<$ count}
                        \State idx $\gets$ index + i
                        \State LEVEL $\gets$ binaryNodes[g].delta/3 - (count - 1 - i)
                        \State ID $\gets$ idx
                        \State PREFIX $\gets$ MCODES[g] $>>$ (30 - (3 * LEVEL)
                        \If{$i > 0$}
                            \State PARENT $\gets$ idx - 1
                            \State childIndex $\gets$ extractBits(PREFIX, 0)
                            \State PARENT.children[childIndex] $\gets$ idx
                        \EndIf
                    \EndFor
                \EndIf
                \State BARRIER
                \If{$\text{count} > 0$}
                    \State testIndex $\gets$ binaryNodes[g].parent
                    \State childIndex = extractBits(PREFIX, 0);
                    \State testCount $\gets$ nodeCounts[testIndex] - nodeCounts[testIndex - 1]
                    \While{testIndex $\neq$ ROOT AND testCount == 0}
                        \State testIndex $\gets$ gpuBinaryTree[testIndex].parent;
                    \EndWhile
                    \If{testIndex $\neq$ ROOT}
                        \State PARENT $\gets$ nodeCounts[testIndex - 1] + 1;
                    \Else
                        \State PARENT $\gets$ ROOT
                    \EndIf
                    \State PARENT.children[childIndex] $\gets$ ID
                \EndIf
            \Else
                \State ID $\gets$ ROOT
                \State LEVEL $\gets$ 0
                \State PREFIX $\gets$ 0
                \State PARENT $\gets$ ROOT   
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}
Octree node generation has an average running time of $O((N \log N_{Bodies}) / P)$, and a worst-case running time of $O(N \times N_{Bodies})$ should the binary tree be completely degenerate, where $N$ is the number of octree nodes, and $N_{Bodies}$ is the number of bodies in the simulation. High levels of binary tree degeneracy are impossible, however, as degenerate particles are removed from the simulation, and a completely degenrate binary tree would contain very few Morton codes. Since the Morton codes are based on the bounding box of the simulation, it is far more likely that we will have a well-distributed tree rather than a degenerate one. In parallel, each
%------------------------------------------------------------------
\subsubsection{Octree Linking}
We now have an incompletely linked octree. In the final step of octree construction, we must complete the node-body relations in the tree. In parallel, we descend the tree based on each particle's Morton code. Once we can no longer descend the tree, we place the particle in the appropriate child slot of the final node. Figure \ref{fig:TreeLinking} shows the linking process of one particle in the tree. In this step we also take our first steps towards dealing with degenerate  Morton codes by only linking the leftmost degenerate child into the octree. Since the particles are sorted by  Morton code, we know that if we have a particle whose  Morton code does not match the particle to its left, then we either have a normal particle or the leftmost of a group of matching-code particles.

\begin{algorithm}
    \label{alg:LinkOctree}
    \caption{Octree hierarchy generation algorithm}
    \begin{algorithmic}
        \For{P in Particles} in parallel
            \If{P.mortonCode $\neq$ [P - 1].mortonCode}
                \State index $\gets$ ROOT
                \State leafFound $\gets$ false
                \State chunkLevel $\gets$ 0
                \While{leafFound is false}
                    \State currentChunk $\gets$ extractBits(P.mortonCode, 9 - chunkLevel)
                    \If{octreeNodes[index].children[currentChunk]}
                        \State index $\gets$ octreeNodes[index].children[currentChunk]
                    \Else
                        \State octreeNodes[index].children[currentChunk] $\gets$ P
                        \State leafFound $\gets$ true
                    \EndIf
                    \State chunkLevel $\gets$ chunkLevel + 1
                \EndWhile
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}
%------------------------------------------------------------------
\subsubsection{Octree Node Statistics} %TODO: REWORK SECTION
Once we have completed our octree hierarchy of particles, we are able to perform the first calculations on the structure. For each node in the octree, we must determine both the center of mass and the total mass enclosed.

During the Node Statistics step, we will also perform necessary calculations such as determining the critical radius of each node and the multipole expansion of each node, which will be used during treecode force calculation should we choose not to run an exact simulation.

These computations will be performed by sweeping up from the leaves of the tree up towards the root of the tree. To finish the multipole expansions, we will sweep back down from the root of the tree to the leaves.

% \begin{algorithm}
%     \label{alg:OctreeNodeStats}
%     \caption{Octree node statistics algorithm}
%     \begin{algorithmic}
%         \For{$i < \text{NUM\_OCTNODES}$}
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}
%------------------------------------------------------------------
\subsubsection{Octree Threading}
To reduce number of divergent operations during force calculation, and to eventually reduce the required memory for octree storage, we thread the octree with \texttt{NEXT} and \texttt{MORE} indices. First, each octree node is assigned a thread on the GPU. Then, in parallel, each thread selects their left most child as the location of the \texttt{MORE} index should that thread be assigned to an internal node. The \texttt{NEXT} index is then selected by examination of the parent hierarchy and the location of the node within the parent nodes's child array. First, the parent node is selected. Next, starting at the location of the child assigned to the thread, the parent node's children array is scanned to the right for the next child. If a child is found before the end of the array, then that child's index is assigned as the next index for the original child node. Should the end of the array be reached before another child has been found, we must ascend to the next level of the tree and repeat the process. Should the root node of the tree be reached, and no right hand child be found, we assign the root index to the \texttt{NEXT} index.

Degenerate particles are also re-included with this step. Particles with  Morton codes which collide are not added to the tree. However we can take advantage of the fact that in a sorted list, all objects with the same  Morton code will be placed next to each other. During tree linking, the leftmost object is attached to the tree. Since this object is already attached, the \texttt{MORE} index of its parent will be computed normally. The difference, however, is that for any object that shares a Morton code with its rightmost neighbor, its rightmost neighbor's index will be used as that particle's \texttt{NEXT} index. Should a particle not share a  Morton code with its rightmost neighbor, we know that we have either a non-degenerate body or that we have reached the end of the group of degenerate  Morton codes. By computing the \texttt{NEXT} and \texttt{MORE} pointers this way, we avoid having to do any special handling of duplicate  Morton codes during tree construction, while still including the bodies in the force calculation. Since these calculations are both independent and close in memory space, it parallelizes very well. Figure \ref{fig:TreeThreading} shows this process in more detail for a 2 dimensional quadtree, although the process is the same for an octree.

\begin{algorithm}
    \label{alg:OctreeThreading}
    \caption{Octree threading algorithm: $O(N/P)$}
    \begin{algorithmic}
        \ForAll{O in Octree Nodes} in parallel
            \State $O.\text{MORE} \gets \text{leftChild(O)}$
            \If{O.mortonCode $=$ rightNeighbor(O).mortonCode}
                \State O.NEXT $\gets$ rightNeighbor(O)
                \State NEXTFOUND $\gets$ true
            \EndIf
            \While{NEXTFOUND is false}
                \State N $\gets$ O.parent
                \If{N.nextChild $\neq$ NULL}
                    \State O.NEXT $\gets$ N.nextChild
                    \State NEXTFOUND $\gets$ true
                \ElsIf{N is ROOT}
                    \State O.NEXT $\gets$ NULL
                    \State NEXTFOUND $\gets$ true
                \EndIf
            \EndWhile
        \EndFor
    \end{algorithmic}
\end{algorithm}
Since the maximum depth of our octree is 10, the worst-case running time of the octree threading kernel is O(10N/P), which becomes O(N/P). In reality this is impossible, as collecting all bodies into one node such that they must traverse to the top of the tree will cause extreme degeneracy, which is handled in O(1) time per node.
%------------------------------------------------------------------
\subsubsection{Force Calculation}
Once the tree has been threaded, it is now time to perform force calculations. Each leaf (particle) node is assigned a thread. All threads then start at the root node, and follow the \texttt{NEXT} and \texttt{MORE} pointers through the tree, calculating the forces enacted by either a cell's center of mass or an individual particle. Should an internal node be far enough away from the original particle, the center of mass of that node is used rather than calculating the forces to all particles within the system. Should the node not meet this criteria, the \texttt{MORE} index is followed to a tree depth which meets this criteria, or until a body node is reached, at which point the \texttt{NEXT} index will be followed and forces calculated like usual. Figure \ref{fig:ForceCalculationTreecode} shows this process in more detail.
%------------------------------------------------------------------
\subsubsection{Integration}
Integration follows the same four step procedure as the brute force algorithm:
\begin{enumerate}[noitemsep]
    \item Advance Half Velocity
    \item Advance Position
    \item Calculate Force
    \item Advance Half Velocity
\end{enumerate}
In fact, the only difference between the brute force GPU algorithm and the Treecode GPU algorithm is how the forces are calculated. Before the integration step, we return all the body node information back to the individualized memory buffers that the brute force algorithm uses, as having individual values next to each other in memory will improve performance and prevents us from having to initialize additional kernels.
%==================================================================
\chapter{RESULTS}
%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
% \section{Statistical}
% \subsection{Brute Force}
% To compare the statistical performance of the brute force simulation, 4096 bodies were run for 10,000 timesteps in a potential well using the same random seed between both the GPU and CPU. The results of this are shown in Figure \ref{fig:BruteForceStats}. 
% \begin{figure}[h]
%     \caption{Statistical comparison between Brute Force GPU and CPU algorithms. Simulation run with 4096 bodies for 10,000 timesteps}
%     \label{fig:BruteForceStats}
%     \centering
%     \includegraphics[width=15cm]{4096Bodies10kTimestep}
% \end{figure}

% \subsection{Treecode}
%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
\section{Performance}
\subsection{Brute Force}
Performance of the brute force algorithm is $O\left(N^2/P\right)$ where $N$ is the number of bodies in a simulation and $P$ is the number of available processors. Shown in Table \ref{tab:BruteForcePerformance} and Figure \ref{fig:BruteForcePerformance} we compare the performance of the GPU and CPU brute force algorithms measured by time to complete 1000 simulation timesteps. The CPU used was a dual-core Intel i5 with 4 available threads. The GPUs used were an NVS 5400M with 96 available CUDA cores, and a GTX Titan Z with 5760 available CUDA cores.
\begin{table}
    \centering
    \caption{Brute Force algorithm performance comparison between CPU and GPU.}
    \label{tab:BruteForcePerformance}
    \begin{tabular}{|c|||c||c|}
        \hline
        \multicolumn{3}{|c|}{Time to Complete 1000 Simulation Timesteps (s)}\\
        \hline
        Bodies & i5-3360M & NVS 5400M \\
        \hline
        1024 & 9 & 3\\
        2048 & 34 & 10\\
        4096 & 158 & 39\\
        8192 & 743 & 168\\
        \hline
    \end{tabular}
\end{table}
\begin{figure}[t]
    \caption{Brute Force algorithm performance comparison between CPU and GPU}
    \label{fig:BruteForcePerformance}
    \centering
    \includegraphics[width=15cm]{BruteForcePerformance}
\end{figure}
As expected, the GPU shows $O(N^2)$ performance with a flatter curve than the CPU due to the higher number of available cores.
\subsection{Tree Construction}
Shown in Figure \ref{fig:treeConstructionPerformance} and Table \ref{tab:treeConstructionPerformance} is a breakdown of GPU vs CPU performance. Figure \ref{fig:treeConstructionProfiling} shows the GPU tree construction profile. While these results are not yet what we would like to see out of the GPU, they do show promise that further optimization can produce even faster results. Examining Figure \ref{fig:treeConstructionProfiling}, we see that the vast majority of the computation time is taken up by the tree construction kernels.
\begin{table}
    \centering
    \caption{Tree Construction Performance comparison between CPU and GPU including COM calculation.}
    \label{tab:treeConstructionPerformance}
    \begin{tabular}{|c|||c||c|c|}
        \hline
        \multicolumn{4}{|c|}{Time to Execute Tree Construction (ms)}\\
        \hline
        Bodies & i5-3360M & NVS5400M & GTX TITAN Z \\
        \hline
        1024 & 0.2248 & 2.6202 & 2.8522\\
        2048 & 0.5300 & 3.9361 & 3.6571\\
        4096 & 1.5059 & 9.3050 & 5.1010\\
        8192 & 4.5760 & 24.3769 & 6.8071\\
        16384 & 7.0729 & 45.9290 & 10.2291\\
        32768 & 14.5459 & 60.8602 & 15.0211\\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[h]
    \caption{Performance comparison between GPU and CPU tree construction algorithms.}
    \label{fig:treeConstructionPerformance}
    \centering
    \includegraphics[width=\textwidth]{treeConstructionPerformance}
\end{figure}
\begin{figure}[h]
    \caption{Profile of GPU tree construction and supporting kernels using an NVIDIA GTX Titan Z GPU.}
    \label{fig:treeConstructionProfiling}
    \centering
    \includegraphics[width=\textwidth]{GTXTITANZTreeProfile}
\end{figure}
We can dig deeper, profiling the individual kernels themselves that make up tree construction. As shown in Figure \ref{fig:TreeCreationProfile}, the creation of the binary tree takes far more time than any of the other kernels, including the octree generation and threading kernels, which can suffer from thread divergence as threads search through tree structures to find nodes that match specific conditions. This is to be expected due to the inefficient COM calculation method.
\begin{figure}[h]
    \caption{Profile of individual GPU tree construction kernels using an NVIDIA GTX Titan Z GPU.}
    \label{fig:TreeCreationProfile}
    \centering
    \includegraphics[width=\textwidth]{TreeCreationProfile}
\end{figure}
We can safely remove COM calculation from the binary tree construction, accepting the fact that we will create a new O(N) upsweep kernel which will compute the COM. Performance of the binary tree construction kernel without COM calculation compared to the binary tree construction kernel with COM calculation is shown in Figure \ref{fig:BinaryTreeComparison}, and the new tree construction kernel profile is shown in Figure \ref{fig:TreeCreationProfileNoCOM}.

\begin{figure}[h]
    \caption{Performance comparison between GPU binary tree construction kernels with and without binary node COM calculation.}
    \label{fig:BinaryTreeComparison}
    \centering
    \includegraphics[width=\textwidth]{BinaryTreeComparison}
\end{figure}
\begin{figure}[h]
    \caption{Profile of individual GPU tree construction kernels after removal of COM calculation from the binary tree creation kernel using an NVIDIA GTX Titan Z GPU.}
    \label{fig:TreeCreationProfileNoCOM}
    \centering
    \includegraphics[width=\textwidth]{TreeCreationProfileNoCOM}
\end{figure}
For a fair comparison, we can remove COM calculation from the CPU tree construction kernel, and reassess GPU vs CPU performance, as shown in Table \ref{tab:treeConstructionPerformanceNoCOM}, and Figure \ref{fig:TreeConstructionPerformanceNoCOM}.
\begin{table}
    \centering
    \caption{Tree Construction Performance comparison between CPU and GPU not including COM calculation.}
    \label{tab:treeConstructionPerformanceNoCOM}
    \begin{tabular}{|c|||c||c|c|}
        \hline
        \multicolumn{4}{|c|}{Time to Execute Tree Construction (ms)}\\
        \hline
        Bodies & i5-3360M & NVS5400M & GTX TITAN Z \\
        \hline
        1024 & 0.2449 & 3.8211 & 2.1040\\
        2048 & 0.4430 & 3.8741 & 2.3830\\
        4096 & 1.2920 & 5.9390 & 2.7809\\
        8192 & 3.6061 & 9.9900 & 3.0560\\
        16384 & 5.8770 & 30.5359 & 4.0941\\
        32768 & 14.7851 & 45.9051 & 6.6700\\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[h]
    \caption{\label{fig:TreeConstructionPerformanceNoCOM} Performance comparison between GPU and CPU tree construction kernels without COM calculation}
    \centering
    \includegraphics[width=\textwidth]{TreeConstructionPerformanceNoCOM}
\end{figure}
%==================================================================
\chapter{FUTURE WORK}
While much has been accomplished at the time of the writing of this paper, there are still a few more things which need to be completed before the project is finished and ready to release to the public. We have implemented a fast method for creation of a threaded octree, and shown tree correctness by performing an exact treecode simulation on both the CPU and GPU. Before GPU N-Body for MilkyWay@Home is ready for public release, the following must be completed:

\section{Efficient COM Calculation}
As seen in the results discussion, inefficient calculation of the COM caused major performance reduction. In order to implement an efficient COM calculation, we can create a kernel which will sweep up the tree level by level, calculating the COM contribution at each layer based on the COM and enclosed mass of each child node. This upsweep kernel will also be used for the future implementation of the Fast Multipole Method.

\section{Fast Multipole Method}
While Barnes-Hut is a step in the right direction, the Fast Multipole Method, first proposed by \cite{GREENGARD1987} provides an even better performance benefit. Fortunately, FMM can utilize the same tree structure, meaning that the majority of the work so far has actually been towards implementation of FMM. Barnes-Hut is an easy checkpoint to prove that the tree is calculated properly. For implementation of FMM, the center of mass upsweep kernel will be used to compute the inner product of the multipole expansion, followed by a downsweep kernel which will compute the outer product as the tree is traversed from the root to the leaves.

\section{Checkpointing for simulation safety}
An important aspect of running large-scale, and especially lengthy simulations is the process of checkpointing. Before GPU N-Body can be released to the volunteer network, checkpointing must be implemented. Because the network uses all volunteer computing power, checkpointing plays an even more important role, as we do not know the condition of the machines we are running on. To implement checkpointing without inducing too much additional load on the GPU, we can take advantage of the fact that openCL uses a queue based system for running kernels. When we wish to checkpoint, we can simply periodically enqueue a blocking buffer read on the device, which will allow the host machine to read data from the device, which can then be written to a checkpoint file.

\section{Replacement of Bitonic sort with Radix sort}
Bitonic sort was initially chosen due to its easy parallelizability, however another sorting algorithm, Radix sort, may offer a performance benefit. \cite{DBLP:journals/corr/BozidarD15} shows a comparison of several different sorting algorithms, with different underlying data distributions. The performance of Radix sort far surpasses that of Bitonic sort, especially for long sequence lengths. As sorting the particles takes up a large amount of the performance profile, it is our hope to implement parallel Radix sort before the release of GPU MilkyWay@Home N-Body to further increase the efficiency of our user's volunteer computing time.
% \section{Reduction of GPU memory by reducing binary and octree buffer size}


%==================================================================
% The following produces a numbered bibliography where the numbers
% correspond to the \cite commands in the text.
% \specialhead{LITERATURE CITED}
\bibliography{bibfile}

%==================================================================
%%%%%%%%%%%%%%%%%%%%%%%  For Appendices  %%%%%%%%%%%%%%%%%%%
\appendix    % This command is used only once!
\addtocontents{toc}{\parindent0pt\vskip12pt APPENDICES} %toc entry, no page #
\chapter{FIGURES}
% ---------------------
% Z-ORDER CURVE
% ---------------------
\begin{figure}[h]
    \caption{\label{fig:MortonCurve} Z-Order curve on sample particle distribution using 6 bits or three levels of resolution for a two dimensional simulation.  Note the degeneracy condition when there is insufficient bit resolution to seperate two particles.}
    \centering
    \includegraphics[width=15cm]{MortonCurve}
\end{figure}
\clearpage
% ---------------------
% BITONIC MORTON SORT
% ---------------------
\begin{figure}[h]
    \caption{\label{fig:BitonicSortResult} Results of sorting an unordered array of particles keyed with  Morton codes. Degenerate particles will always end up next to each other in the final sorted array as they share a  Morton code.}
    \centering
    \includegraphics[width=10cm]{BitonicSortResult}
\end{figure}
\clearpage
% ---------------------
% COMPACT MORTON CODES
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{CompactMortonCodes}
    \caption{\label{fig:CompactMortonCodes} Process outline of compaction of degenerate morton codes by marking and calculating a prefix sum offset array.}
\end{sidewaysfigure}
\clearpage
% ---------------------
% BINARY TREE GENERATION
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{BinaryTreeGeneration}
    \caption{\label{fig:BinaryTreeGeneration} Generation of the binary tree from the underlying Morton code structure. Note how each node is placed directly above its corresponding Morton code.}
\end{sidewaysfigure}
\clearpage
% ---------------------
% OCTREE NODE CONTRIBUTION
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{OctreeNodeContribution}
    \caption{\label{fig:OctreeNodeContribution} Calculation of the octree node contribution array using the same marking and parallel prefix sum technique as the compaction algorithm, except the NC array gives the location of each of the octree nodes for a given binary tree node.}
\end{sidewaysfigure}
\clearpage
% ---------------------
% OCT NODE GENERATION
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{OctreeNodeGeneration}
    \caption{\label{fig:OctreeNodeGeneration} Creation of octree nodes and strands from the node counts array. Note that strands are always created in contiguous memory space, which allows us to compute offsets more easily when attaching children.}
\end{sidewaysfigure}
\clearpage
% ---------------------
% OCTREE LINKING
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{TreeLinking}
    \caption{\label{fig:TreeLinking} Linking process shown for one body. In this step, bodies are linked with the overall octree structure. Note how in groups of degenerate particles, only the leftmost particle is added to the hierarchy.}
\end{sidewaysfigure}
\clearpage
% ---------------------
% OCTREE THREADING
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{TreeThreading}
    \caption{\label{fig:TreeThreading} Threading of a quadtree including a degenerate  Morton code condition. Black connectors represent parent-child relations, while red arrows represent NEXT and MORE relations. Arrows stemming from M correspond to MORE and arrows stemming from N correspond to NEXT.}
\end{sidewaysfigure}
\clearpage
% ---------------------
% FORCE CALCULATION
% ---------------------
\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{ForceCalculationTreecode}
    \caption{\label{fig:ForceCalculationTreecode} Threading of a quadtree including a degenerate  Morton code condition. Black connectors represent parent-child relations, while red arrows represent NEXT and MORE relations. Arrows stemming from M correspond to MORE and arrows stemming from N correspond to NEXT.}
\end{sidewaysfigure}
\clearpage


% %==================================================================
% \chapter{THIS IS ANOTHER APPENDIX}
% This is a sentence to take up space and look like text.

\end{document}
